---
job: extension
config:
  # This name will be the folder for checkpoints/samples and the base filename for the LoRA.
  name: "hidream_i1_finetune"
  # Root folder to save all training outputs.
  training_folder: "output"
  device: 'cuda'
  # For tensorboard logging.
  log_dir: "output/.tensorboard"
  process:
    - type: 'sd_trainer'
      # The base model for fine-tuning.
      model:
        # Use the full, non-distilled version for maximum quality.
        name_or_path: "HiDream-ai/HiDream-I1-Full"
        # CRITICAL: Tells the toolkit to use FLUX-compatible logic for the VAE.
        is_flux: true
        # Quantizes the model to 8-bit during loading to fit in VRAM.
        quantize: false  # Set to false for L40S with 48GB VRAM
        # Set to true only if your training GPU is also driving your monitor.
        low_vram: false
      # LoRA network configuration.
      network:
        type: "lora"
        # Moderate rank for good quality/efficiency balance
        rank: 16
        # Standard practice is to set alpha equal to rank
        alpha: 16
        # Surgical targeting of DiT attention and MLP layers, excluding text encoders.
        target_modules:
          - "q_proj"
          - "k_proj"
          - "v_proj"
          - "o_proj"
          - "gate_proj"
          - "down_proj"
          - "up_proj"
      # Optimizer settings.
      optimizer:
        type: "adamw8bit"  # Using 8-bit AdamW for memory efficiency
        # A balanced learning rate for a large model and constant scheduler.
        learning_rate: 1.0e-4  # 0.0001
        # A simple and stable scheduler is preferred for isolating LR impact.
        lr_scheduler: "constant"
        lr_warmup_steps: 0
      # Core training parameters.
      train:
        # Total number of training steps
        steps: 2000
        # Batch size - can increase to 4 for L40S
        batch_size: 2
        # Gradient accumulation for effective larger batch size
        gradient_accumulation_steps: 2
        # bf16 is essential for stability and performance on modern GPUs.
        dtype: 'bf16'
        # Save checkpoints periodically to evaluate and avoid overfitting.
        save_every: 500
        # Maximum intermediate saves to keep
        max_step_saves_to_keep: 4
        # Skip the pre-training sample
        skip_first_sample: true
        # Disable sampling since we don't want to generate images during training
        disable_sampling: true
        # Enable gradient checkpointing for memory efficiency
        gradient_checkpointing: true
        # Noise scheduler for HiDream
        noise_scheduler: "flowmatch"
        # Timestep type
        timestep_type: "shift"
        # Train only the UNet, not text encoder
        train_unet: true
        train_text_encoder: false
        # Recommended to leave on for smoother learning.
        ema_config:
          use_ema: true
          ema_decay: 0.99
        # WandB logging configuration
        use_wandb: true
        wandb_project: "hidream-finetune"
        wandb_run_name: "hidream_i1_lora"
      # Dataset configuration.
      dataset:
        # A list of datasets can be provided.
        datasets:
          - folder_path: "/workspace/ai-toolkit/assets"
            caption_ext: "txt"
            # Small dropout for robustness
            caption_dropout_rate: 0.05
            # Do not shuffle tokens in structured captions.
            shuffle_tokens: false
            # Cache latents for faster training
            cache_latents_to_disk: true
            # Multiple resolutions for HiDream
            resolution: [512, 768, 1024]
      # Sample generation disabled - uncomment if you want to generate samples
      # sample:
      #   # Sampler must be flowmatch for FLUX/HiDream architectures.
      #   sampler: "flowmatch"
      #   # Generate samples frequently to monitor progress.
      #   sample_every: 250
      #   width: 1024
      #   height: 1024
      #   prompts:
      #     - "hidream_still_edito, your prompt here"
      
      # Additional configuration
      trigger_word: "hidream_still_edito"
      performance_log_every: 100
      
      # Save settings
      save:
        dtype: bfloat16  # precision to save
        save_every: 500  # save every this many steps
        max_step_saves_to_keep: 4  # how many intermittent saves to keep
      
      # WandB logging configuration (alternative location)
      wandb:
        enabled: true
        project: "hidream-finetune"
        run_name: "hidream_i1_lora"
        tags: ["hidream", "lora", "finetune"]
        log_every: 10  # log metrics every N steps
        log_images: false  # no images since sampling is disabled
        save_model: true  # save model checkpoints to wandb

# Additional metadata
meta:
  name: "hidream_i1_finetune"
  version: '1.0'
  description: "HiDream-I1 LoRA training configuration optimized for L40S"