# HiDream-I1 LoRA Training Configuration
# Optimized for RunPod L40S (48GB VRAM) or similar GPUs
# Using HiDream-I1-Full model for best quality training

---
job: extension
config:
  # this name will be the folder and filename name
  name: "hidream_i1_finetune"
  process:
    - type: 'sd_trainer'
      # root folder to save training sessions/samples/weights
      training_folder: "output"
      # uncomment to see performance stats in the terminal every N steps
      performance_log_every: 100
      device: cuda:0
      # trigger word that will be added to captions if not present
      trigger_word: "hidream_still_edito"
      network:
        type: "lora"
        linear: 16
        linear_alpha: 16
        network_kwargs:
          # ignore mixture of experts as recommended
          ignore_if_contains:
            - "ff_i.experts"
            - "ff_i.gate"
      save:
        dtype: bfloat16  # precision to save
        save_every: 500  # save every this many steps
        max_step_saves_to_keep: 4  # how many intermittent saves to keep
      datasets:
        # using the dataset folder with paired images and captions
        - folder_path: "dataset"
          caption_ext: "txt"
          caption_dropout_rate: 0.05  # will drop out the caption 5% of time
          shuffle_tokens: false
          cache_latents_to_disk: true
          resolution: [512, 768, 1024]  # hidream enjoys multiple resolutions
      train:
        batch_size: 2  # can increase to 4 for L40S
        steps: 2000  # total number of steps to train
        gradient_accumulation_steps: 2
        train_unet: true
        train_text_encoder: false  # won't work with hidream
        gradient_checkpointing: true  # needed unless you have tons of vram
        noise_scheduler: "flowmatch"  # for training only
        timestep_type: shift  # sigmoid, shift, linear
        optimizer: "adamw8bit"
        lr: 1e-4
        # skip the pre training sample
        skip_first_sample: true
        # disable sampling since we don't have prompts
        disable_sampling: true
        
        # ema will smooth out learning
        ema_config:
          use_ema: true
          ema_decay: 0.99
        
        # bf16 is required for hidream
        dtype: bf16
        
        # WandB logging in train section
        use_wandb: true
        wandb_project: "hidream-finetune"
        wandb_run_name: "hidream_i1_lora"
      model:
        # Using HiDream-I1-Full for best quality (requires more VRAM)
        # For less VRAM, use "HiDream-ai/HiDream-I1-Fast" but quality may suffer
        name_or_path: "HiDream-ai/HiDream-I1-Full"
        extras_name_or_path: "HiDream-ai/HiDream-I1-Full"
        arch: "hidream"
        # Quantization settings - adjust based on VRAM
        # L40S (48GB): can set both to false for better quality
        # 24GB cards: keep both true
        quantize: false  # set to true if less than 40GB VRAM
        quantize_te: true  # text encoder quantization
        model_kwargs:
          # llama model for text encoding
          llama_model_path: "unsloth/Meta-Llama-3.1-8B-Instruct"
      # Sampling disabled - no prompts needed
      # sample:
      #   sampler: "flowmatch"
      #   sample_every: 250
      #   width: 1024
      #   height: 1024
      
      # WandB logging configuration
      wandb:
        enabled: true
        project: "hidream-finetune"
        run_name: "hidream_i1_lora"
        tags: ["hidream", "lora", "finetune"]
        log_every: 10  # log metrics every N steps
        log_images: false  # no images since sampling is disabled
        save_model: true  # save model checkpoints to wandb

# Additional metadata
meta:
  name: "[name]"
  version: '1.0'
  description: "HiDream-I1 LoRA training configuration"